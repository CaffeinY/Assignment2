{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/cse158_assignment2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMhmMb_7CmR9",
        "outputId": "49461495-5367-451a-c0c8-b97a54363bfa"
      },
      "id": "PMhmMb_7CmR9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/cse158_assignment2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301bf995",
      "metadata": {
        "id": "301bf995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c09f4365-088b-4b4a-cb26-9a338fb354b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import scipy.optimize\n",
        "from sklearn import svm\n",
        "import numpy\n",
        "import string\n",
        "import random\n",
        "import string\n",
        "from sklearn import linear_model\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = gzip.open(\"reviews_Beauty_5.json.gz\")\n",
        "dataset = []\n",
        "for l in f:\n",
        "  dataset.append(json.loads(l))\n",
        "print(len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfLfdfXxeaKM",
        "outputId": "6d73df94-f8a7-49a8-9e4f-ba00b2dea12b"
      },
      "id": "CfLfdfXxeaKM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [d for d in dataset if d[\"overall\"] != 3]\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyiCPetSfTLD",
        "outputId": "08c7132e-4ef7-4cc2-ff10-3dabfa9d4ca9"
      },
      "id": "yyiCPetSfTLD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176254\n",
            "{'reviewerID': 'A1YJEY40YUW4SE', 'asin': '7806397051', 'reviewerName': 'Andrea', 'helpful': [3, 4], 'reviewText': 'Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.', 'overall': 1.0, 'summary': \"Don't waste your money\", 'unixReviewTime': 1391040000, 'reviewTime': '01 30, 2014'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in dataset:\n",
        "  if d[\"overall\"] >3:\n",
        "    d[\"positive_rating\"] = 1\n",
        "  else:\n",
        "    d[\"positive_rating\"] = 0\n",
        "\n",
        "dataset_train = dataset[:140000]\n",
        "dataset_test = dataset[140000:]\n",
        "\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxE_ht49fq8E",
        "outputId": "bba12f10-358d-4c6b-ce2b-a0f7cdbc6587"
      },
      "id": "pxE_ht49fq8E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'reviewerID': 'A1YJEY40YUW4SE', 'asin': '7806397051', 'reviewerName': 'Andrea', 'helpful': [3, 4], 'reviewText': 'Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.', 'overall': 1.0, 'summary': \"Don't waste your money\", 'unixReviewTime': 1391040000, 'reviewTime': '01 30, 2014', 'positive_rating': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))\n",
        "print(sum([d[\"positive_rating\"] for d in dataset]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmuB1OXDi4OO",
        "outputId": "dff8af6e-2b2e-42ff-c661-e85c5a5bac77"
      },
      "id": "KmuB1OXDi4OO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176254\n",
            "154272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wordCount = defaultdict(int)\n",
        "punctuation = set(string.punctuation)\n",
        "for d in dataset_train:\n",
        "  r = ''.join([c for c in d['reviewText'].lower() if not c in punctuation])\n",
        "  for w in r.split():\n",
        "    # if w in stop_words:\n",
        "    #   continue\n",
        "    wordCount[w] += 1\n",
        "\n",
        "counts = [(wordCount[w], w) for w in wordCount]\n",
        "print(len(counts))\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "words = [x[1] for x in counts[:2000]]\n",
        "print(words[:10])\n",
        "\n",
        "wordId = dict(zip(words,range(len(words))))\n",
        "wordSet = set(words)\n",
        "\n",
        "def feature(datnum):\n",
        "  r = ''.join([c for c in datnum['reviewText'].lower() if not c in punctuation])\n",
        "  f = [0]*len(words)\n",
        "  counter = 0\n",
        "  for w in r.split():\n",
        "    if w in wordSet:\n",
        "      f[wordId[w]] += 1\n",
        "      counter+=1\n",
        "  return f + [1]\n",
        "\n",
        "x_train = [feature(d) for d in dataset_train]\n",
        "y_train = [d[\"positive_rating\"] for d in dataset_train]\n",
        "\n",
        "\n",
        "mod = linear_model.LogisticRegression(C=1)\n",
        "mod.fit(x_train,y_train)\n",
        "\n",
        "x_test = [feature(d) for d in dataset_test]\n",
        "y_test = [d[\"positive_rating\"] for d in dataset_test]\n",
        "\n",
        "pred = mod.predict(x_test)\n",
        "print(len(pred))\n",
        "\n",
        "print(pred[:10])\n",
        "print(y_test[:10])\n",
        "\n",
        "def accuracy(predictions, y):\n",
        "  TP = sum([(p == False and l == False) for (p,l) in zip(predictions,y)])\n",
        "  FP = sum([(p == False and l == True) for (p,l) in zip(predictions,y)])\n",
        "  TN = sum([(p == True and l == True) for (p,l) in zip(predictions,y)])\n",
        "  FN = sum([(p == True and l == False) for (p,l) in zip(predictions,y)])\n",
        "  return (TP+TN)/(TP+FP+TN+FN)\n",
        "\n",
        "print(accuracy(pred, y_test))\n",
        "print(accuracy([1]*len(y_test),y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbeWWx2weaZa",
        "outputId": "fe7b0625-00e5-4713-f22a-e6f6c6e9f81c"
      },
      "id": "SbeWWx2weaZa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142131\n",
            "['i', 'the', 'and', 'it', 'a', 'to', 'this', 'my', 'is', 'of']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36254\n",
            "[1 1 1 0 1 1 1 1 1 1]\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "0.9381861311855244\n",
            "0.9059414133612843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################### change cut off to 4 ##########################################################"
      ],
      "metadata": {
        "id": "JxghZmMjiOkD"
      },
      "id": "JxghZmMjiOkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = gzip.open(\"reviews_Beauty_5.json.gz\")\n",
        "dataset = []\n",
        "for l in f:\n",
        "  dataset.append(json.loads(l))\n",
        "print(len(dataset))\n",
        "dataset = [d for d in dataset if d[\"overall\"] != 4]\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset[0])\n",
        "\n",
        "\n",
        "for d in dataset:\n",
        "  if d[\"overall\"] >4:\n",
        "    d[\"positive_rating\"] = 1\n",
        "  else:\n",
        "    d[\"positive_rating\"] = 0\n",
        "\n",
        "dataset_train = dataset[:130000]\n",
        "dataset_test = dataset[130000:]\n",
        "\n",
        "print(len(dataset))\n",
        "print(sum([d[\"positive_rating\"] for d in dataset]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl9MfVEbl4co",
        "outputId": "5dd35f8b-0064-4974-9394-a6102224656c"
      },
      "id": "bl9MfVEbl4co",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198502\n",
            "158761\n",
            "{'reviewerID': 'A1YJEY40YUW4SE', 'asin': '7806397051', 'reviewerName': 'Andrea', 'helpful': [3, 4], 'reviewText': 'Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.', 'overall': 1.0, 'summary': \"Don't waste your money\", 'unixReviewTime': 1391040000, 'reviewTime': '01 30, 2014'}\n",
            "158761\n",
            "114531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "wordCount = defaultdict(int)\n",
        "punctuation = set(string.punctuation)\n",
        "for d in dataset_train:\n",
        "  r = ''.join([c for c in d['reviewText'].lower() if not c in punctuation])\n",
        "  for w in r.split():\n",
        "    # if w in stop_words:\n",
        "    #   continue\n",
        "    wordCount[w] += 1\n",
        "\n",
        "counts = [(wordCount[w], w) for w in wordCount]\n",
        "print(len(counts))\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "words = [x[1] for x in counts[:2000]]\n",
        "print(words[:10])\n",
        "\n",
        "wordId = dict(zip(words,range(len(words))))\n",
        "wordSet = set(words)\n",
        "\n",
        "def feature(datnum):\n",
        "  r = ''.join([c for c in datnum['reviewText'].lower() if not c in punctuation])\n",
        "  f = [0]*len(words)\n",
        "  counter = 0\n",
        "  for w in r.split():\n",
        "    if w in wordSet:\n",
        "      f[wordId[w]] += 1\n",
        "      counter+=1\n",
        "  return f + [1]\n",
        "\n",
        "x_train = [feature(d) for d in dataset_train]\n",
        "y_train = [d[\"positive_rating\"] for d in dataset_train]\n",
        "\n",
        "\n",
        "mod = linear_model.LogisticRegression(C=1)\n",
        "mod.fit(x_train,y_train)\n",
        "\n",
        "x_test = [feature(d) for d in dataset_test]\n",
        "y_test = [d[\"positive_rating\"] for d in dataset_test]\n",
        "\n",
        "pred = mod.predict(x_test)\n",
        "print(len(pred))\n",
        "\n",
        "print(pred[:10])\n",
        "print(y_test[:10])\n",
        "\n",
        "def accuracy(predictions, y):\n",
        "  TP = sum([(p == False and l == False) for (p,l) in zip(predictions,y)])\n",
        "  FP = sum([(p == False and l == True) for (p,l) in zip(predictions,y)])\n",
        "  TN = sum([(p == True and l == True) for (p,l) in zip(predictions,y)])\n",
        "  FN = sum([(p == True and l == False) for (p,l) in zip(predictions,y)])\n",
        "  return (TP+TN)/(TP+FP+TN+FN)\n",
        "\n",
        "print(accuracy(pred, y_test))\n",
        "print(accuracy([1]*len(y_test),y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYx7IqZBl9cE",
        "outputId": "f52aa36f-fea6-4a44-e46b-6462f232241a"
      },
      "id": "jYx7IqZBl9cE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133606\n",
            "['i', 'the', 'and', 'it', 'a', 'to', 'this', 'my', 'is', 'of']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28761\n",
            "[0 1 1 1 0 1 1 1 1 1]\n",
            "[0, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n",
            "0.8800111261778102\n",
            "0.7375265115955635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BFjT1YumwVC"
      },
      "id": "_BFjT1YumwVC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}